{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b24926cb",
   "metadata": {},
   "source": [
    "# HuggingFace Genomics Hands‚ÄëOn Exercise\n",
    "## DNA Sequence Analysis with Foundation Models\n",
    "\n",
    "**Author:** Ikram Ullah, KAUST Bioinformatics Platform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64299fba",
   "metadata": {},
   "source": [
    "## Setup (1/2) Install and import all required Python packages\n",
    "\n",
    "We install PyTorch (CUDA 12.1 wheels if NVIDIA GPU is available), visualization and ML libraries, and Hugging Face tooling so the rest of the notebook runs without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f3e946-40e3-4c11-8a65-792feedd080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "#!pip install matplotlib scikit-learn ipython \n",
    "#!pip install transformers umap-learn evaluate \n",
    "#!pip uninstall -y triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52eebbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ee20e",
   "metadata": {},
   "source": [
    "## Setup (2/2)\n",
    "**What & Why:** Select a compute device. We prefer GPU if available for speed; otherwise the code falls back to CPU automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1de0b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3500481e",
   "metadata": {},
   "source": [
    "## Part 1: PyTorch Tensors (1/3)\n",
    "**What & Why:** Convert a DNA string into integer indices (A/T/C/G ‚Üí 0/1/2/3). This is a simple way to map symbolic sequences into numeric tensors for downstream models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84452401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ATCGATCGATCG\n",
      "Tensor: tensor([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3])\n",
      "Shape: torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "dna = \"ATCGATCGATCG\"\n",
    "mapping = {'A': 0, 'T': 1, 'C': 2, 'G': 3}\n",
    "\n",
    "encoded = [mapping[b] for b in dna]\n",
    "tensor = torch.tensor(encoded)\n",
    "\n",
    "print(f\"Original: {dna}\")\n",
    "print(f\"Tensor: {tensor}\")\n",
    "print(f\"Shape: {tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a45e78",
   "metadata": {},
   "source": [
    "## Part 1: PyTorch Tensors (2/3)\n",
    "**What & Why:** One‚Äëhot encode a DNA sequence (length √ó 4). One‚Äëhot vectors are a common baseline representation for nucleotides and enable simple batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23b7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(seq):\n",
    "    mapping = {'A': 0, 'T': 1, 'C': 2, 'G': 3}\n",
    "    one_hot = torch.zeros(len(seq), 4)\n",
    "    for i, b in enumerate(seq):\n",
    "        one_hot[i, mapping[b]] = 1\n",
    "    return one_hot\n",
    "\n",
    "encoded = one_hot_encode(\"ATCG\")\n",
    "print(encoded.shape)  # (4, 4)\n",
    "\n",
    "seqs = [\"ATCG\", \"GCTA\"]\n",
    "batch = torch.stack([one_hot_encode(s) for s in seqs])\n",
    "print(batch.shape)  # (2, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abcb9be",
   "metadata": {},
   "source": [
    "## Part 1: PyTorch Tensors (3/3)\n",
    "**What & Why:** Demonstrate moving tensors between CPU and GPU. Keeping model and tensors on the same device is essential for correct and fast computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f75104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Back to: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "t = torch.randn(100, 512)\n",
    "t_gpu = t.to(device)\n",
    "print(\"Device:\", t_gpu.device)\n",
    "\n",
    "t_cpu = t_gpu.cpu()\n",
    "print(\"Back to:\", t_cpu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cbf125",
   "metadata": {},
   "source": [
    "## Part 2: Load Models (1/2) ‚Äî DNABERT‚Äë2\n",
    "**What & Why:** Load DNABERT‚Äë2 with `trust_remote_code=True` and its repo config. This avoids config‚Äëclass mismatches when custom remote code is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f975a265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# üß¨ **DNABERT-2 Model Summary**\n",
       "\n",
       "| Property | Description |\n",
       "|:--|:--|\n",
       "| **Model ID** | `zhihan1996/DNABERT-2-117M` |\n",
       "| **Architecture** | **BERT + ALiBi (Attention with Linear Biases)** |\n",
       "| **Tokenizer Type** | `PreTrainedTokenizerFast` |\n",
       "| **Vocab Size** | **4,096** |\n",
       "| **Max Sequence Length** | **512** |\n",
       "| **Hidden Size** | **768** |\n",
       "| **Number of Layers** | **12** |\n",
       "| **Attention Heads** | **12** |\n",
       "| **Total Parameters** | üß† **117,068,544** |\n",
       "\n",
       "> ‚ÑπÔ∏è *Source:* [Hugging Face Model Card](https://huggingface.co/zhihan1996/DNABERT-2-117M)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, BertConfig\n",
    "from termcolor import colored\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "model_name = \"zhihan1996/DNABERT-2-117M\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "config = BertConfig.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, config=config)\n",
    "\n",
    "# Compute parameters\n",
    "params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Prepare styled Markdown output\n",
    "info_md = f\"\"\"\n",
    "# üß¨ **DNABERT-2 Model Summary**\n",
    "\n",
    "| Property | Description |\n",
    "|:--|:--|\n",
    "| **Model ID** | `{model_name}` |\n",
    "| **Architecture** | **BERT + ALiBi (Attention with Linear Biases)** |\n",
    "| **Tokenizer Type** | `{type(tokenizer).__name__}` |\n",
    "| **Vocab Size** | **{tokenizer.vocab_size:,}** |\n",
    "| **Max Sequence Length** | **{config.max_position_embeddings}** |\n",
    "| **Hidden Size** | **{config.hidden_size}** |\n",
    "| **Number of Layers** | **{config.num_hidden_layers}** |\n",
    "| **Attention Heads** | **{config.num_attention_heads}** |\n",
    "| **Total Parameters** | üß† **{params:,}** |\n",
    "\n",
    "> ‚ÑπÔ∏è *Source:* [Hugging Face Model Card](https://huggingface.co/zhihan1996/DNABERT-2-117M)\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(info_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55101e1a",
   "metadata": {},
   "source": [
    "## Part 2: Load Models (2/2) ‚Äî Nucleotide Transformer\n",
    "**What & Why:** Load InstaDeep‚Äôs Nucleotide Transformer as an alternative foundation model. We‚Äôll later compare model sizes and use either for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7021a003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-500m-human-ref and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# üß¨ **Nucleotide Transformer (Human-Ref, 500M) Summary**\n",
       "\n",
       "| Property | Description |\n",
       "|:--|:--|\n",
       "| **Model ID** | `InstaDeepAI/nucleotide-transformer-500m-human-ref` |\n",
       "| **Architecture** | **Transformer (BERT-like, RoFormer variant)** |\n",
       "| **Pretraining Objective** | Masked Language Modeling (on reference genomes) |\n",
       "| **Tokenizer Type** | `EsmTokenizer` |\n",
       "| **Vocab Size** | **4,107** |\n",
       "| **Max Sequence Length** | **1002** |\n",
       "| **Hidden Size** | **1280** |\n",
       "| **Number of Layers** | **24** |\n",
       "| **Attention Heads** | **20** |\n",
       "| **Total Parameters** | üß† **480,438,241** |\n",
       "| **Training Data** | Human reference genome (GRCh38) + RefSeq |\n",
       "| **Key Features** | Large-scale self-supervised pretraining, cross-species generalization |\n",
       "| **Paper / Preprint** | [Biorxiv: *Nucleotide Transformers for Genomics*](https://www.biorxiv.org/content/10.1101/2023.01.11.523679v2) |\n",
       "\n",
       "> ‚öôÔ∏è *Implements a scalable Transformer backbone for genomics, trained on billions of nucleotides.*\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nt_name = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "nt_tokenizer = AutoTokenizer.from_pretrained(nt_name, trust_remote_code=True)\n",
    "nt_config = AutoConfig.from_pretrained(nt_name)\n",
    "nt_model = AutoModel.from_pretrained(nt_name, trust_remote_code=True).to(device).eval()\n",
    "\n",
    "# Count parameters\n",
    "nt_params = sum(p.numel() for p in nt_model.parameters())\n",
    "\n",
    "nt_md = f\"\"\"\n",
    "# üß¨ **Nucleotide Transformer (Human-Ref, 500M) Summary**\n",
    "\n",
    "| Property | Description |\n",
    "|:--|:--|\n",
    "| **Model ID** | `{nt_name}` |\n",
    "| **Architecture** | **Transformer (BERT-like, RoFormer variant)** |\n",
    "| **Pretraining Objective** | Masked Language Modeling (on reference genomes) |\n",
    "| **Tokenizer Type** | `{type(nt_tokenizer).__name__}` |\n",
    "| **Vocab Size** | **{nt_tokenizer.vocab_size:,}** |\n",
    "| **Max Sequence Length** | **{nt_config.max_position_embeddings}** |\n",
    "| **Hidden Size** | **{nt_config.hidden_size}** |\n",
    "| **Number of Layers** | **{nt_config.num_hidden_layers}** |\n",
    "| **Attention Heads** | **{nt_config.num_attention_heads}** |\n",
    "| **Total Parameters** | üß† **{nt_params:,}** |\n",
    "| **Training Data** | Human reference genome (GRCh38) + RefSeq |\n",
    "| **Key Features** | Large-scale self-supervised pretraining, cross-species generalization |\n",
    "| **Paper / Preprint** | [Biorxiv: *Nucleotide Transformers for Genomics*](https://www.biorxiv.org/content/10.1101/2023.01.11.523679v2) |\n",
    "\n",
    "> ‚öôÔ∏è *Implements a scalable Transformer backbone for genomics, trained on billions of nucleotides.*\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(nt_md))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01fa8de-1c40-4353-9fc7-a3655362f77b",
   "metadata": {},
   "source": [
    "## Part 3: Hugging Face Platform\n",
    "**What & Why:** Explore https://huggingface.co/models to discover genomic models (e.g., DNABERT‚Äë2 and Nucleotide Transformer). Read model cards to learn tokenization, max sequence length, architecture, and intended uses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a772983-30fe-484b-b46c-f67e156d470a",
   "metadata": {},
   "source": [
    "## Part 3: Hugging Face Platform (Browse the Model Hub)\n",
    "\n",
    "* **Visit** ‚Üí https://huggingface.co/models\n",
    "\n",
    "* **Search examples**:\n",
    "    * \"DNABERT\" ‚Üí zhihan1996/DNABERT-2-117M\n",
    "    * \"nucleotide transformer\" ‚Üí InstaDeepAI variants\n",
    "    * \"genomic\" ‚Üí multiple models\n",
    "* **Explore**:\n",
    "    * Model ID\n",
    "    * Download count\n",
    "    * Tokenization method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b49272d-f59a-4921-956e-ad6b4cc3ae36",
   "metadata": {},
   "source": [
    "## Part 3: Hugging Face Platform (Exercise 2.2: Model Card Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4342f81a-3ffe-48ca-9968-40f5638a200b",
   "metadata": {},
   "source": [
    "* **Visit** ‚Üí [DNABERT-2 Model Card](https://huggingface.co/zhihan1996/DNABERT-2-117M)\n",
    "* **Find**\n",
    "    * Tokenization ‚Üí BPE\n",
    "    * Max seq length ‚Üí 512\n",
    "    * Architecture ‚Üí BERT + ALiBi\n",
    "    * Parameters ‚Üí 117M\n",
    "* **Model Card Sections**\n",
    "    * Overview\n",
    "    * Model Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dcb35b",
   "metadata": {},
   "source": [
    "## Part 4: Hugging Face Inference API (1/3) ‚Äî Setup Authentication\n",
    "**What & Why:** Store your personal access token (read access) to call the hosted Inference API for models without local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2aa44",
   "metadata": {},
   "outputs": [],
   "source": "HF_TOKEN = \"your_token_here\"  # Replace with your token from https://huggingface.co/settings/tokens"
  },
  {
   "cell_type": "markdown",
   "id": "ad5ec51c",
   "metadata": {},
   "source": [
    "## Part 4: Hugging Face Inference API (2/3) ‚Äî Query the API\n",
    "**What & Why:** Define a helper that posts a sequence to the Inference API and returns JSON output. This is handy for quick prototypes or when GPUs aren‚Äôt available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e502c942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def query_api(seq, model_id, token):\n",
    "    \"\"\"\n",
    "    Query the Hugging Face Inference API for a given model and DNA sequence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seq : str\n",
    "        Input DNA sequence with <mask> token (e.g. \"ATCGATCG<mask>ATCG\").\n",
    "    model_id : str\n",
    "        Hugging Face model ID.\n",
    "    token : str\n",
    "        Hugging Face access token with read permissions.\n",
    "    \"\"\"\n",
    "    url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    response = requests.post(url, headers=headers, json={\"inputs\": seq})\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå API Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "    try:\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Response not in JSON format:\", e)\n",
    "        print(response.text[:300])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5496d6bc-e90c-427b-80d8-5ea3e84b7a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Input Sequence: ATCGATCG<mask>ATCG\n",
      "üß† Model: InstaDeepAI/nucleotide-transformer-500m-human-ref\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_76433 th {\n",
       "  background-color: #f2f2f2;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_76433_row0_col1 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #a6cee3 100.0%, transparent 100.0%);\n",
       "}\n",
       "#T_76433_row1_col1 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #a6cee3 72.6%, transparent 72.6%);\n",
       "}\n",
       "#T_76433_row2_col1 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #a6cee3 72.3%, transparent 72.3%);\n",
       "}\n",
       "#T_76433_row3_col1 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #a6cee3 70.2%, transparent 70.2%);\n",
       "}\n",
       "#T_76433_row4_col1 {\n",
       "  width: 10em;\n",
       "  background: linear-gradient(90deg, #a6cee3 66.4%, transparent 66.4%);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_76433\">\n",
       "  <caption>Top-5 model predictions for the masked DNA region</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_76433_level0_col0\" class=\"col_heading level0 col0\" >Predicted DNA motif (replacing <mask>)</th>\n",
       "      <th id=\"T_76433_level0_col1\" class=\"col_heading level0 col1\" >Confidence score</th>\n",
       "      <th id=\"T_76433_level0_col2\" class=\"col_heading level0 col2\" >Reconstructed sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_76433_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_76433_row0_col0\" class=\"data row0 col0\" ><b style='color:#1f78b4'>AGCCTT</b></td>\n",
       "      <td id=\"T_76433_row0_col1\" class=\"data row0 col1\" >3.342%</td>\n",
       "      <td id=\"T_76433_row0_col2\" class=\"data row0 col2\" >ATCGAT C G<b style='color:#33a02c'> AGCCTT</b> A T C G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_76433_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_76433_row1_col0\" class=\"data row1 col0\" ><b style='color:#1f78b4'>TTTCCT</b></td>\n",
       "      <td id=\"T_76433_row1_col1\" class=\"data row1 col1\" >2.425%</td>\n",
       "      <td id=\"T_76433_row1_col2\" class=\"data row1 col2\" >ATCGAT C G<b style='color:#33a02c'> TTTCCT</b> A T C G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_76433_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_76433_row2_col0\" class=\"data row2 col0\" ><b style='color:#1f78b4'>CAGCTT</b></td>\n",
       "      <td id=\"T_76433_row2_col1\" class=\"data row2 col1\" >2.418%</td>\n",
       "      <td id=\"T_76433_row2_col2\" class=\"data row2 col2\" >ATCGAT C G<b style='color:#33a02c'> CAGCTT</b> A T C G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_76433_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_76433_row3_col0\" class=\"data row3 col0\" ><b style='color:#1f78b4'>CGTCTT</b></td>\n",
       "      <td id=\"T_76433_row3_col1\" class=\"data row3 col1\" >2.347%</td>\n",
       "      <td id=\"T_76433_row3_col2\" class=\"data row3 col2\" >ATCGAT C G<b style='color:#33a02c'> CGTCTT</b> A T C G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_76433_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_76433_row4_col0\" class=\"data row4 col0\" ><b style='color:#1f78b4'>CTTTTC</b></td>\n",
       "      <td id=\"T_76433_row4_col1\" class=\"data row4 col1\" >2.220%</td>\n",
       "      <td id=\"T_76433_row4_col2\" class=\"data row4 col2\" >ATCGAT C G<b style='color:#33a02c'> CTTTTC</b> A T C G</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "masked_seq = \"ATCGATCG<mask>ATCG\"\n",
    "model_id = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "result = query_api(masked_seq, model_id, HF_TOKEN)\n",
    "\n",
    "# ---- Display pedagogic output ----\n",
    "if result:\n",
    "    df = pd.DataFrame(result)[[\"token_str\", \"score\", \"sequence\"]]\n",
    "    df.rename(columns={\n",
    "        \"token_str\": \"Predicted DNA motif (replacing <mask>)\",\n",
    "        \"score\": \"Confidence score\",\n",
    "        \"sequence\": \"Reconstructed sequence\"\n",
    "    }, inplace=True)\n",
    "\n",
    "    print(f\"üî¨ Input Sequence: {masked_seq}\")\n",
    "    print(f\"üß† Model: {model_id}\\n\")\n",
    "\n",
    "    display(HTML(df.style\n",
    "        .bar(subset=[\"Confidence score\"], color='#a6cee3')\n",
    "        .format({\n",
    "            \"Confidence score\": \"{:.3%}\",\n",
    "            \"Predicted DNA motif (replacing <mask>)\": lambda x: f\"<b style='color:#1f78b4'>{x}</b>\",\n",
    "            \"Reconstructed sequence\": lambda x: x.replace(x[10:17], f\"<b style='color:#33a02c'>{x[10:17]}</b>\")\n",
    "        })\n",
    "        .set_table_styles([\n",
    "            {'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('font-weight', 'bold')]}\n",
    "        ])\n",
    "        .set_caption(\"Top-5 model predictions for the masked DNA region\")\n",
    "        .to_html()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e202b-2ede-4838-9806-6c39c45f3b0d",
   "metadata": {},
   "source": [
    "##### Each row shows a possible 6-mer that the model believes fits in the masked position, ordered by confidence. The model reconstructs the complete sequence accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0b8486",
   "metadata": {},
   "source": [
    "## Part 4: Hugging Face Inference API (3/3) ‚Äî Compare Local vs API\n",
    "**What & Why:** Time a minimal local forward pass vs a remote API call. This illustrates the latency trade‚Äëoffs between local GPU and hosted inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd5832b-fa6d-4b35-8789-479124441a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Using model: InstaDeepAI/nucleotide-transformer-500m-human-ref on cuda\n",
      "\n",
      "üìò Part 1: Embedding Extraction (AutoModel)\n",
      "\n",
      "‚úÖ Embedding extracted ‚Äî shape: (1, 1280)\n",
      "\n",
      "‚öôÔ∏è Part 2: Local MLM Prediction (True softmax over full vocab)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dcb8d\">\n",
       "  <caption>üß© Local MLM Predictions (Normalized)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dcb8d_level0_col0\" class=\"col_heading level0 col0\" >Rank</th>\n",
       "      <th id=\"T_dcb8d_level0_col1\" class=\"col_heading level0 col1\" >Predicted 6-mer</th>\n",
       "      <th id=\"T_dcb8d_level0_col2\" class=\"col_heading level0 col2\" >Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb8d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_dcb8d_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_dcb8d_row0_col1\" class=\"data row0 col1\" >AGCCTT</td>\n",
       "      <td id=\"T_dcb8d_row0_col2\" class=\"data row0 col2\" >3.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb8d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_dcb8d_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_dcb8d_row1_col1\" class=\"data row1 col1\" >TTTCCT</td>\n",
       "      <td id=\"T_dcb8d_row1_col2\" class=\"data row1 col2\" >2.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb8d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_dcb8d_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_dcb8d_row2_col1\" class=\"data row2 col1\" >CAGCTT</td>\n",
       "      <td id=\"T_dcb8d_row2_col2\" class=\"data row2 col2\" >2.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb8d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_dcb8d_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_dcb8d_row3_col1\" class=\"data row3 col1\" >CGTCTT</td>\n",
       "      <td id=\"T_dcb8d_row3_col2\" class=\"data row3 col2\" >2.35%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcb8d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_dcb8d_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_dcb8d_row4_col1\" class=\"data row4 col1\" >CTTTTC</td>\n",
       "      <td id=\"T_dcb8d_row4_col2\" class=\"data row4 col2\" >2.22%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚òÅÔ∏è Part 3: Remote Inference via Hugging Face API\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_64ad2\">\n",
       "  <caption>‚òÅÔ∏è Hugging Face API Predictions</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_64ad2_level0_col0\" class=\"col_heading level0 col0\" >Rank</th>\n",
       "      <th id=\"T_64ad2_level0_col1\" class=\"col_heading level0 col1\" >Predicted 6-mer</th>\n",
       "      <th id=\"T_64ad2_level0_col2\" class=\"col_heading level0 col2\" >Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_64ad2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_64ad2_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_64ad2_row0_col1\" class=\"data row0 col1\" >AGCCTT</td>\n",
       "      <td id=\"T_64ad2_row0_col2\" class=\"data row0 col2\" >3.34%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64ad2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_64ad2_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_64ad2_row1_col1\" class=\"data row1 col1\" >TTTCCT</td>\n",
       "      <td id=\"T_64ad2_row1_col2\" class=\"data row1 col2\" >2.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64ad2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_64ad2_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_64ad2_row2_col1\" class=\"data row2 col1\" >CAGCTT</td>\n",
       "      <td id=\"T_64ad2_row2_col2\" class=\"data row2 col2\" >2.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64ad2_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_64ad2_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_64ad2_row3_col1\" class=\"data row3 col1\" >CGTCTT</td>\n",
       "      <td id=\"T_64ad2_row3_col2\" class=\"data row3 col2\" >2.35%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_64ad2_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_64ad2_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_64ad2_row4_col1\" class=\"data row4 col1\" >CTTTTC</td>\n",
       "      <td id=\"T_64ad2_row4_col2\" class=\"data row4 col2\" >2.22%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import logging, time\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# üîß 1Ô∏è‚É£ Setup\n",
    "# ------------------------------------------------------------------\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
    "print(f\"üß† Using model: {model_id} on {device}\\n\")\n",
    "\n",
    "# Load tokenizer once\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# üìò 2Ô∏è‚É£ Embedding Extraction (AutoModel)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"üìò Part 1: Embedding Extraction (AutoModel)\\n\")\n",
    "\n",
    "embed_model = AutoModel.from_pretrained(model_id, trust_remote_code=True).to(device).eval()\n",
    "\n",
    "seq_plain = [\"ATCGATCGATCG\"]\n",
    "tokens_plain = tokenizer(seq_plain, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = embed_model(**tokens_plain)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "print(f\"‚úÖ Embedding extracted ‚Äî shape: {embeddings.shape}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# ‚öôÔ∏è 3Ô∏è‚É£ Local MLM Prediction (AutoModelForMaskedLM)\n",
    "# ------------------------------------------------------------------\n",
    "print(\"‚öôÔ∏è Part 2: Local MLM Prediction (True softmax over full vocab)\\n\")\n",
    "\n",
    "# Load the MLM-capable model\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(model_id, trust_remote_code=True).to(device).eval()\n",
    "\n",
    "# Input sequence with a mask token\n",
    "seq_masked = [\"ATCGATCG<mask>ATCG\"]\n",
    "tokens_masked = tokenizer(seq_masked, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "t0 = time.time()\n",
    "with torch.no_grad():\n",
    "    outputs = mlm_model(**tokens_masked)\n",
    "    logits = outputs.logits\n",
    "t1 = time.time()\n",
    "local_time = t1 - t0\n",
    "\n",
    "# Find mask token location and extract logits\n",
    "mask_token_index = (tokens_masked[\"input_ids\"] == tokenizer.mask_token_id).nonzero(as_tuple=True)\n",
    "mask_logits = logits[mask_token_index]\n",
    "\n",
    "# Compute probabilities over the full vocab\n",
    "probs = F.softmax(mask_logits, dim=-1)\n",
    "top_k = torch.topk(probs, 5, dim=-1)\n",
    "top_tokens = top_k.indices[0].tolist()\n",
    "top_scores = top_k.values[0].tolist()\n",
    "\n",
    "# Build readable table\n",
    "preds_local = [\n",
    "    {\"Rank\": i+1, \"Predicted 6-mer\": tokenizer.decode([t]).strip(), \"Confidence\": f\"{s*100:.2f}%\"}\n",
    "    for i, (t, s) in enumerate(zip(top_tokens, top_scores))\n",
    "]\n",
    "\n",
    "display(HTML(pd.DataFrame(preds_local).style.set_caption(\"üß© Local MLM Predictions (Normalized)\").to_html()))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# ‚òÅÔ∏è 4Ô∏è‚É£ Remote Hugging Face API Prediction\n",
    "# ------------------------------------------------------------------\n",
    "print(\"‚òÅÔ∏è Part 3: Remote Inference via Hugging Face API\\n\")\n",
    "\n",
    "def query_api(seq, model_id, token):\n",
    "    \"\"\"Query the Hugging Face Inference API for predictions.\"\"\"\n",
    "    url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    r = requests.post(url, headers=headers, json={\"inputs\": seq})\n",
    "    if r.status_code != 200:\n",
    "        print(f\"‚ùå API Error {r.status_code}: {r.text[:300]}\")\n",
    "        return None\n",
    "    try:\n",
    "        return r.json()\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Non-JSON response:\", e, r.text[:300])\n",
    "        return None\n",
    "\n",
    "t0 = time.time()\n",
    "api_result = query_api(seq_masked[0], model_id, HF_TOKEN)\n",
    "t1 = time.time()\n",
    "api_time = t1 - t0\n",
    "\n",
    "if api_result:\n",
    "    preds_api = [\n",
    "        {\"Rank\": i+1, \"Predicted 6-mer\": r[\"token_str\"], \"Confidence\": f\"{r['score']*100:.2f}%\"}\n",
    "        for i, r in enumerate(api_result[:5])\n",
    "    ]\n",
    "    display(HTML(pd.DataFrame(preds_api).style.set_caption(\"‚òÅÔ∏è Hugging Face API Predictions\").to_html()))\n",
    "else:\n",
    "    print(\"API call failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "349a0b73-5524-452d-b173-5f18e1a726c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4d637 th {\n",
       "  background-color: #f2f2f2;\n",
       "  font-weight: bold;\n",
       "  text-align: center;\n",
       "  border: 1px solid #ccc;\n",
       "}\n",
       "#T_4d637 td {\n",
       "  text-align: center;\n",
       "  padding: 6px;\n",
       "  border: 1px solid #eee;\n",
       "}\n",
       "#T_4d637_row0_col3, #T_4d637_row1_col3 {\n",
       "  font-weight: bold;\n",
       "  color: #1f77b4;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4d637\">\n",
       "  <caption>‚öñÔ∏è Local vs Hugging Face Inference Comparison</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4d637_level0_col0\" class=\"col_heading level0 col0\" >Inference Mode</th>\n",
       "      <th id=\"T_4d637_level0_col1\" class=\"col_heading level0 col1\" >Environment</th>\n",
       "      <th id=\"T_4d637_level0_col2\" class=\"col_heading level0 col2\" >Input Example</th>\n",
       "      <th id=\"T_4d637_level0_col3\" class=\"col_heading level0 col3\" >Avg Runtime (s)</th>\n",
       "      <th id=\"T_4d637_level0_col4\" class=\"col_heading level0 col4\" >Key Advantages</th>\n",
       "      <th id=\"T_4d637_level0_col5\" class=\"col_heading level0 col5\" >Limitations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4d637_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4d637_row0_col0\" class=\"data row0 col0\" >Local (GPU/CPU)</td>\n",
       "      <td id=\"T_4d637_row0_col1\" class=\"data row0 col1\" >Local runtime / GPU node</td>\n",
       "      <td id=\"T_4d637_row0_col2\" class=\"data row0 col2\" >ATCGATCGATCG</td>\n",
       "      <td id=\"T_4d637_row0_col3\" class=\"data row0 col3\" >0.116</td>\n",
       "      <td id=\"T_4d637_row0_col4\" class=\"data row0 col4\" >‚ö° Fast, customizable, runs offline</td>\n",
       "      <td id=\"T_4d637_row0_col5\" class=\"data row0 col5\" >Requires installation + GPU/CPU resources</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4d637_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4d637_row1_col0\" class=\"data row1 col0\" >Hugging Face API (Cloud)</td>\n",
       "      <td id=\"T_4d637_row1_col1\" class=\"data row1 col1\" >Hugging Face Inference Service</td>\n",
       "      <td id=\"T_4d637_row1_col2\" class=\"data row1 col2\" >ATCGATCG<mask>ATCG</td>\n",
       "      <td id=\"T_4d637_row1_col3\" class=\"data row1 col3\" >0.410</td>\n",
       "      <td id=\"T_4d637_row1_col4\" class=\"data row1 col4\" >‚òÅÔ∏è No setup, accessible anywhere</td>\n",
       "      <td id=\"T_4d637_row1_col5\" class=\"data row1 col5\" >Network latency + model availability constraints</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Assume you already have: local_time, api_time, model_id\n",
    "comparison_data = {\n",
    "    \"Inference Mode\": [\"Local (GPU/CPU)\", \"Hugging Face API (Cloud)\"],\n",
    "    \"Environment\": [\"Local runtime / GPU node\", \"Hugging Face Inference Service\"],\n",
    "    \"Input Example\": [\"ATCGATCGATCG\", \"ATCGATCG<mask>ATCG\"],\n",
    "    \"Avg Runtime (s)\": [f\"{local_time:.3f}\", f\"{api_time:.3f}\"],\n",
    "    \"Key Advantages\": [\n",
    "        \"‚ö° Fast, customizable, runs offline\",\n",
    "        \"‚òÅÔ∏è No setup, accessible anywhere\"\n",
    "    ],\n",
    "    \"Limitations\": [\n",
    "        \"Requires installation + GPU/CPU resources\",\n",
    "        \"Network latency + model availability constraints\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "styled = (\n",
    "    df.style\n",
    "    .set_table_styles([\n",
    "        {\"selector\": \"th\", \"props\": [\n",
    "            (\"background-color\", \"#f2f2f2\"),\n",
    "            (\"font-weight\", \"bold\"),\n",
    "            (\"text-align\", \"center\"),\n",
    "            (\"border\", \"1px solid #ccc\")\n",
    "        ]},\n",
    "        {\"selector\": \"td\", \"props\": [\n",
    "            (\"text-align\", \"center\"),\n",
    "            (\"padding\", \"6px\"),\n",
    "            (\"border\", \"1px solid #eee\")\n",
    "        ]}\n",
    "    ])\n",
    "    .set_caption(\"‚öñÔ∏è Local vs Hugging Face Inference Comparison\")\n",
    "    .set_properties(subset=[\"Avg Runtime (s)\"], **{\"font-weight\": \"bold\", \"color\": \"#1f77b4\"})\n",
    ")\n",
    "\n",
    "display(HTML(styled.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570c4a66-7db4-4709-bb0e-090c4a30db0f",
   "metadata": {},
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e1247-82f7-4eaa-9246-c40c3a22a8dc",
   "metadata": {},
   "source": [
    "## Part 5: HF Datasets + Nucleotide Transformer (Run ‚Üí Inspect ‚Üí Answer)\n",
    "\n",
    "### Objectives (read only)\n",
    "1. Authenticate to HF Platform\n",
    "2. Load a genomics dataset from the Hub (already split).\n",
    "3. Pin a revision for reproducibility.\n",
    "4. Explore schema, class balance, and sequence stats.\n",
    "5. Tokenize with a Nucleotide Transformer tokenizer.\n",
    "6. Fine-tune a classifier head; evaluate.\n",
    "7. Push artifacts back to the Hub and run hosted inference.\n",
    "\n",
    "**Prereqs (once)**:\n",
    "\n",
    "    * pip install -q transformers datasets evaluate huggingface_hub scikit-learn\n",
    "    * You‚Äôll also need a free Hugging Face account + access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c60b72-23a5-458f-b204-34346762ce26",
   "metadata": {},
   "source": [
    "### Part 5: HF Datasets (1/7: Load genomics dataset)\n",
    "\n",
    "Login to Hugging Face Platform by running below from terminal. \n",
    "\n",
    "* _You should have account on HuggingFace to proceed with this part._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d714bc2-26bd-487f-ba30-2362ccc8c6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "The token `test` has been saved to /biocorelab/BIX/resources/hf_resources/stored_tokens\n",
      "Your token has been saved to /biocorelab/BIX/resources/hf_resources/token\n",
      "Login successful.\n",
      "The current active token is: `test`\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade huggingface_hub\n",
    "!source ~/.bashrc\n",
    "!huggingface-cli login --token \"$HF_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5cc008c-84b7-4af5-b756-79579eb3b9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.49.0\n",
      "Datasets: 3.6.0\n",
      "Python: 3.11.13\n",
      "HF user/org: ullahi\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "import transformers, datasets, sys, ipywidgets\n",
    "\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Datasets:\", datasets.__version__)\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "\n",
    "api = HfApi()\n",
    "print(\"HF user/org:\", api.whoami()[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3045e6-71c2-425a-bb04-d33600dd5ff2",
   "metadata": {},
   "source": [
    "### Part 5: HF Datasets (2/7: Load a pre-split dataset from the Hub)\n",
    "\n",
    "We will use the dataset _InstaDeepAI/nucleotide_transformer_downstream_tasks_ for this session. It has been released by InstaDeep AI team. \n",
    "\n",
    "The dataset collects 18 downstream classification tasks (both binary and multi‚Äêclass) that were used to evaluate the Nucleotide Transformer models. Each task corresponds to a genomic regulatory or structural element (for example: promoter detection, enhancer classification, splice-site prediction, histone mark prediction) in human DNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58444d35-0520-4ec0-9f8e-dfa05af41b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e474ce0959c54d8c85f58067742874a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724614c510cd4f7581012ad48bed53d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'name', 'label', 'task'],\n",
       "        num_rows: 461850\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'name', 'label', 'task'],\n",
       "        num_rows: 48797\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Pin a revision for reproducibility (use \"main\" or a commit hash if you have one)\n",
    "REV = \"main\"\n",
    "ds_all = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks\", revision=REV)\n",
    "ds_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04764166-4ecd-4069-a559-d0d79c060c2b",
   "metadata": {},
   "source": [
    "### Pick one task and inspect\n",
    "The dataset holds multiple tasks; we‚Äôll choose one (e.g., \"promoter_all\"). For sake of testing, we take a subset of the data to speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0613e9b3-1add-49e5-9f6c-ac25bd5be06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['H3', 'H3K14ac', 'H3K36me3', 'H3K4me1', 'H3K4me2', 'H3K4me3',\n",
       "       'H3K79me3', 'H3K9ac', 'H4', 'H4ac', 'enhancers', 'enhancers_types',\n",
       "       'promoter_all', 'promoter_no_tata', 'promoter_tata',\n",
       "       'splice_sites_acceptors', 'splice_sites_all',\n",
       "       'splice_sites_donors'], dtype='<U22')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(ds_all['train']['task'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72fa536c-166e-4556-98b9-112f6edf8dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': Value(dtype='string', id=None), 'name': Value(dtype='string', id=None), 'label': Value(dtype='int32', id=None), 'task': Value(dtype='string', id=None)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Counter({0: 7531, 1: 7469}), 15000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "TASK = \"promoter_all\"  # try others later!\n",
    "ds_full = ds_all.filter(lambda ex: ex[\"task\"] == TASK)\n",
    "\n",
    "# take a subset (15K training and 9K testing sample\n",
    "ds = DatasetDict({\n",
    "    \"train\": ds_full[\"train\"].shuffle(seed=42).select(range(15000)),\n",
    "    \"test\":  ds_full[\"test\"].shuffle(seed=42).select(range(5000))\n",
    "})\n",
    "\n",
    "{split: len(ds[split]) for split in ds}\n",
    "print(ds[\"train\"].features)\n",
    "\n",
    "y = [ex[\"label\"] for ex in ds[\"train\"]]\n",
    "Counter(y), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466813a3-60d3-4fa6-90a9-b910d91f5364",
   "metadata": {},
   "source": [
    "#### How many sequences are in training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c958bfb-1d73-4f0e-8695-84c3277e0731",
   "metadata": {},
   "source": [
    "#### Sequence length stats \n",
    "* Sanity check\n",
    "* What‚Äôs the max sequence length in your sample? _Use this to choose max_length for tokenization_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf3e89af-2c4a-47df-adfb-1e1eaf45743f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length: 300, Mean Length: 300.0, Maximum Length: 300\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lengths = np.array([len(ex[\"sequence\"]) for ex in ds[\"train\"].select(range(min(5000, len(ds[\"train\"]))))])\n",
    "print(f\"Minimum length: {lengths.min()}, Mean Length: {lengths.mean()}, Maximum Length: {lengths.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57282c6-2363-4239-b00b-01fdbd716d0d",
   "metadata": {},
   "source": [
    "#### How many classes does this task have? Is the class balance skewed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1528cef-8a5d-4272-b121-409d61e034f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({0: 7531, 1: 7469}), 15000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [ex[\"label\"] for ex in ds[\"train\"]]\n",
    "Counter(y), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17d88828-d131-4d6f-889e-950ef225113e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Label     |   Train Count |   Test Count |   Total |\n",
       "|:----------|--------------:|-------------:|--------:|\n",
       "| 0         |          7531 |         2507 |   10038 |\n",
       "| 1         |          7469 |         2493 |    9962 |\n",
       "| **Total** |         15000 |         5000 |   20000 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Count labels in train and test\n",
    "train_counts = Counter([ex[\"label\"] for ex in ds[\"train\"]])\n",
    "test_counts  = Counter([ex[\"label\"] for ex in ds[\"test\"]])\n",
    "\n",
    "# Create a summary DataFrame\n",
    "labels = sorted(set(train_counts.keys()) | set(test_counts.keys()))\n",
    "df_summary = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Train Count\": [train_counts.get(lbl, 0) for lbl in labels],\n",
    "    \"Test Count\": [test_counts.get(lbl, 0) for lbl in labels],\n",
    "})\n",
    "df_summary[\"Total\"] = df_summary[\"Train Count\"] + df_summary[\"Test Count\"]\n",
    "\n",
    "# Add totals row at the bottom\n",
    "total_row = pd.DataFrame({\n",
    "    \"Label\": [\"**Total**\"],\n",
    "    \"Train Count\": [df_summary[\"Train Count\"].sum()],\n",
    "    \"Test Count\": [df_summary[\"Test Count\"].sum()],\n",
    "    \"Total\": [df_summary[\"Total\"].sum()]\n",
    "})\n",
    "df_summary = pd.concat([df_summary, total_row], ignore_index=True)\n",
    "\n",
    "# Display as Markdown table\n",
    "display(Markdown(df_summary.to_markdown(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2429a405-194e-444e-85df-15ce29f2506a",
   "metadata": {},
   "source": [
    "#### Tokenize using Nucleotide Transformer Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffb69f91-26b8-4ba6-8cc3-5d58350e501b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of the tokenizer is 75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16c0260e8aa4ec3aff011f3189225a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d28bc0f1234427794e2ebccdcd8ed46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 13500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import DatasetDict\n",
    "\n",
    "MODEL_ID = \"InstaDeepAI/nucleotide-transformer-v2-50m-3mer-multi-species\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "print(f\"Vocabulary size of the tokenizer is {tokenizer.vocab_size}\")\n",
    "\n",
    "MAX_LEN = 512  # adjust if your sequences are much longer\n",
    "def preprocess(batch):\n",
    "    toks = tokenizer(batch[\"sequence\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "    toks[\"labels\"] = batch[\"label\"]\n",
    "    return toks\n",
    "\n",
    "tokenized = DatasetDict({k: v.map(preprocess, batched=True, remove_columns=v.column_names)\n",
    "                         for k, v in ds.items()})\n",
    "\n",
    "if \"validation\" not in tokenized:\n",
    "    split = tokenized[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "    tokenized = DatasetDict({\n",
    "        \"train\": split[\"train\"],\n",
    "        \"validation\": split[\"test\"],\n",
    "        \"test\": tokenized[\"test\"]\n",
    "    })\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "13a697f8-e0e8-4c07-b13a-e80afb2051cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmTokenizer(name_or_path='InstaDeepAI/nucleotide-transformer-v2-50m-3mer-multi-species', vocab_size=75, model_max_length=2048, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1da3e4a4-b163-4d23-bfe7-a002f893b26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['train'][0]['input_ids'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f405cce8-681c-4426-97dd-e568810c3667",
   "metadata": {},
   "source": [
    "#### Create a classifier head on top of NT encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97583ad6-149c-4fcb-b383-94e1775dc81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in NT model is 51.73 million\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "num_labels = len(set(ds[\"train\"][\"label\"]))\n",
    "config = AutoConfig.from_pretrained(MODEL_ID, num_labels=num_labels, trust_remote_code=True)\n",
    "\n",
    "# AutoModelForSequenceClassification automatically adds a classification head on top of the base model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, config=config, trust_remote_code=True)\n",
    "\n",
    "# parameter count\n",
    "paramInMillions = round(sum(p.numel() for p in model.parameters())/1e6, 2)\n",
    "print(f\"Number of parameters in NT model is {paramInMillions} million\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d821e1-b835-4c3d-8046-6decb9c0b640",
   "metadata": {},
   "source": [
    "####  Train (quick run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f603aab-15ce-4ea5-bb15-66b73707a298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990c1c5dd64649d38e6544cc075723d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"nt_promoter_run\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,           # keep short for class\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_out = trainer.train()\n",
    "eval_out = trainer.evaluate()\n",
    "eval_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988880cd-f68c-4ea4-b479-51b6f21ff28f",
   "metadata": {},
   "source": [
    "####  Test set evaluation & confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36117b-7e86-4470-a51d-8cba27de4936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "pred = trainer.predict(tokenized[\"test\"])\n",
    "y_true = pred.label_ids\n",
    "y_pred = pred.predictions.argmax(-1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35a862-6cbc-4a11-afc3-8d07dc595b76",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "Try to experiment with different options like: \n",
    "1. Changing task (TASK)\n",
    "2. Adjusting MAX_LEN\n",
    "3. Trying a larger/smaller NT model\n",
    "4. Tweaking LR/epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}