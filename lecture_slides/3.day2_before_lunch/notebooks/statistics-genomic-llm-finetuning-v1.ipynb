{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metadata",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<div style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em; padding: 0.5em;\">\n",
    "\n",
    "# Fine-tuning a LLM for DNA Sequence Classification\n",
    "\n",
    "by [Raphael Mourad](https://training.galaxyproject.org/hall-of-fame/raphaelmourad/), [B√©r√©nice Batut](https://training.galaxyproject.org/hall-of-fame/bebatut/)\n",
    "\n",
    "CC-BY licensed content from the [Galaxy Training Network](https://training.galaxyproject.org/)\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "- How to classify a DNA sequence depending on if it binds a protein or not (transcription factor)?\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "- Load a pre-trained model and modify its architecture to include a classification layer.\n",
    "- Prepare and preprocess labeled DNA sequences for fine-tuning.\n",
    "- Define and configure training parameters to optimize the model's performance on the classification task.\n",
    "- Evaluate the fine-tuned model's accuracy and robustness in distinguishing between different classes of DNA sequences.\n",
    "\n",
    "**Time Estimation: 3H**\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>After preparing, training, and utilizing a language model for DNA sequences, we can now fine-tune a pre-trained Large Language Model (LLM) for specific DNA sequence classification tasks. Here, we will use a pre-trained model from Hugging Face, specifically the <a href=\"https://huggingface.co/RaphaelMourad/Mistral-DNA-v1-17M-hg38\">Mistral-DNA-v1-17M-hg38</a>, and adapt it to classify DNA sequences based on their biological functions. Our objective is to classify sequences according to whether they bind to transcription factors.</p>\n",
    "<blockquote class=\"comment\" style=\"border: 2px solid #ffecc1; margin: 1em 0.2em\">\n",
    "<div class=\"box-title comment-title\" id=\"comment-transcription-factors\"><i class=\"far fa-comment-dots\" aria-hidden=\"true\" ></i> Comment: Transcription factors</div>\n",
    "<p>Transcription factors are proteins that play a crucial role in regulating gene expression by binding to specific DNA sequences, known as enhancers or promoters. These proteins act as molecular switches, turning genes on or off in response to various cellular signals and environmental cues. By binding to DNA, transcription factors either promote or inhibit the recruitment of RNA polymerase, the enzyme responsible for transcribing DNA into RNA, thereby influencing the rate of transcription.</p>\n",
    "<figure id=\"figure-1\" style=\"max-width: 90%;\"><img src=\"images/two_dna_sequences.png\" alt=\"Diagram illustrating DNA binding with CTCF. The left panel, outlined in red, shows a DNA sequence 'CCACCAGGGGGCGC' labeled as 'DNA binding CTCF,' with an oval labeled 'CTCF' above it. The right panel, outlined in blue, shows a different DNA sequence 'GTGGCTAGTAGGTAG' labeled as 'DNA not binding CTCF,' indicating that this sequence does not interact with CTCF.\" width=\"2486\" height=\"852\" loading=\"lazy\" /><a target=\"_blank\" href=\"images/two_dna_sequences.png\" rel=\"noopener noreferrer\"><small>Open image in new tab</small></a><br /><br /><figcaption><span class=\"figcaption-prefix\"><strong>Figure 1</strong>:</span> Two types of DNA sequences. On the left, a DNA sequence that binds the transcription factor CTCF. On the right, a DNA sequence that does not bind CTCF.</figcaption></figure>\n",
    "<p>Transcription factors are essential for numerous biological processes, including cell differentiation, development, and response to external stimuli. Their ability to recognize and bind specific DNA sequences allows them to orchestrate complex gene expression programs, ensuring that the right genes are expressed at the right time and in the right place within an organism. Understanding the function and regulation of transcription factors is vital for deciphering the molecular mechanisms underlying health and disease, and it opens avenues for developing targeted therapeutic interventions.</p>\n",
    "</blockquote>\n",
    "<p>This classification task is crucial for understanding gene regulation, as transcription factors play a vital role in controlling which genes are expressed in a cell. By training a model to predict whether a DNA sequence binds to a transcription factor, we can gain insights into regulatory mechanisms and potentially identify novel binding sites or understand the impact of genetic variations on transcription factor binding.</p>\n",
    "<p>By fine-tuning the model, we aim to leverage its pre-trained knowledge of DNA sequences to achieve high accuracy in this classification task. This tutorial will guide you through the necessary steps, from data preparation to model evaluation, ensuring you can apply these techniques to your own research or projects.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"comment\" style=\"border: 2px solid #ffecc1; margin: 1em 0.2em\">\n",
    "<div class=\"box-title comment-title\" id=\"comment-pretraining-a-llm\"><i class=\"far fa-comment-dots\" aria-hidden=\"true\" ></i> Comment: Pretraining a LLM</div>\n",
    "<p>To learn how to pretrain a LLM on DNA, please follow the dedicated <a href=\"{% link topics/statistics/tutorials/genomic-llm-pretraining/tutorial.md %}\">‚ÄúPretraining a Large Language Model (LLM) from Scratch on DNA Sequences‚Äù</a> tutorial</p>\n",
    "</blockquote>\n",
    "<blockquote class=\"agenda\" style=\"border: 2px solid #86D486;display: none; margin: 1em 0.2em\">\n",
    "<div class=\"box-title agenda-title\" id=\"agenda\">Agenda</div>\n",
    "<p>In this tutorial, we will cover:</p>\n",
    "<ol id=\"markdown-toc\">\n",
    "<li><a href=\"#prepare-resources\" id=\"markdown-toc-prepare-resources\">Prepare resources</a>    <ol>\n",
    "<li><a href=\"#install-dependencies\" id=\"markdown-toc-install-dependencies\">Install dependencies</a></li>\n",
    "</ol>\n",
    "</li>\n",
    "</ol>\n",
    "</blockquote>\n",
    "<h1 id=\"prepare-resources\">Prepare resources</h1>\n",
    "<h2 id=\"install-dependencies\">Install dependencies</h2>\n",
    "<p>The first step is to install the required dependencies:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7a532e-5d36-4bd1-bbdc-2bed2dbc7a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading module for CUDA 12.4.1\u001b[m\n",
      "CUDA 12.4.1 is now loaded\u001b[m\n",
      "\u001b[K\u001b[?1l\u001b>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvcc: command not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#!module add cuda/12.4.1\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starting automated setup for Genomic LLM Fine-tuning...\n",
      "\n",
      "‚úì uv is already installed\n",
      "\n",
      "üîß Creating isolated Python 3.11 environment...\n",
      "Using CPython \u001b[36m3.11.14\u001b[39m\u001b[36m\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv-genomic-llm-course\u001b[39m\n",
      "Activate with: \u001b[32msource .venv-genomic-llm-course/bin/activate\u001b[39m\n",
      "‚úì Environment created: .venv-genomic-llm-course (Python 3.11)\n",
      "\n",
      "üìö Installing packages (this may take 1-3 minutes)...\n",
      "\u001b[2mUsing Python 3.11.14 environment at: .venv-genomic-llm-course\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m80 packages\u001b[0m \u001b[2min 3.43s\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/80] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m80 packages\u001b[0m \u001b[2min 1m 21s\u001b[0m\u001b[0m                             \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1masttokens\u001b[0m\u001b[2m==3.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.48.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.11.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdebugpy\u001b[0m\u001b[2m==1.8.17\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdecorator\u001b[0m\u001b[2m==5.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mexecuting\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython\u001b[0m\u001b[2m==9.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipython-pygments-lexers\u001b[0m\u001b[2m==1.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-core\u001b[0m\u001b[2m==5.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib-inline\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnest-asyncio\u001b[0m\u001b[2m==1.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.3.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mparso\u001b[0m\u001b[2m==0.8.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpexpect\u001b[0m\u001b[2m==4.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprogressbar2\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprompt-toolkit\u001b[0m\u001b[2m==3.0.52\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mptyprocess\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpure-eval\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpygments\u001b[0m\u001b[2m==2.19.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-utils\u001b[0m\u001b[2m==3.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyzmq\u001b[0m\u001b[2m==27.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mstack-data\u001b[0m\u001b[2m==0.6.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtornado\u001b[0m\u001b[2m==6.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtraitlets\u001b[0m\u001b[2m==5.14.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwcwidth\u001b[0m\u001b[2m==0.2.14\u001b[0m\n",
      "\n",
      "‚úì Package installation complete!\n",
      "\n",
      " Setting up Jupyter kernel...\n",
      "Installed kernelspec genomic-llm-course in /home/biosc05/.local/share/jupyter/kernels/genomic-llm-course\n",
      "\n",
      " Setup complete!\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      " NEXT STEPS:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "1. Open the notebook: statistics-genomic-llm-finetuning.ipynb\n",
      "\n",
      "2. Select the kernel:\n",
      "   ‚Üí Click on 'Select Kernel' in the top-right\n",
      "   ‚Üí Choose: Python (Genomic LLM Course)\n",
      "\n",
      "3. Start running the cells!\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      " All packages are installed in: .venv-genomic-llm-course/\n",
      "‚ö° Installation was done with uv (10-100x faster than pip!)\n",
      "\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      " NOTES:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "‚úì GPU detected - ready for training!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üéØ ONE-CLICK SETUP - Just run this cell and everything will be ready!\n",
    "# This cell runs the setup_environment.sh script to:\n",
    "# 1. Install uv (if not already installed)\n",
    "# 2. Create an isolated Python environment\n",
    "# 3. Install all required packages\n",
    "# 4. Set up the kernel for this notebook\n",
    "#\n",
    "# The -y flag automatically replaces any existing environment without asking\n",
    "\n",
    "#!bash setup_environment.sh -y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3b",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"tip\" style=\"border: 2px solid #86D486; margin: 1em 0.2em\">\n",
    "<div class=\"box-title tip-title\" id=\"tip-kernel-change\"><i class=\"fa fa-lightbulb\" aria-hidden=\"true\" ></i> Important: Change Kernel</div>\n",
    "<p>After running the setup cell above, you <strong>must</strong> change the kernel to use the newly created environment:</p>\n",
    "<ol>\n",
    "<li>Click on <strong>Kernel</strong> in the menu bar</li>\n",
    "<li>Select <strong>Change Kernel</strong></li>\n",
    "<li>Choose <strong>Python (Genomic LLM)</strong></li>\n",
    "</ol>\n",
    "<p>Then proceed to run the verification cell below.</p>\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-3c",
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Python version: 3.10.18 | packaged by conda-forge | (main, Jun  4 2025, 14:45:41) [GCC 13.3.0]\n",
      "üìç Python location: /home/ullahi/Downloads/Training.v2/env/bin/python\n",
      "\n",
      "üì¶ Checking installed packages...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ullahi/Downloads/Training.v2/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì accelerate: 1.1.0\n",
      "‚úì peft: 0.13.2\n",
      "‚ö†Ô∏è  torch: 2.5.0+cu121 (expected 2.5.0)\n",
      "‚úì transformers: 4.57.3\n",
      "‚úì progressbar: 2.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ullahi/Downloads/Training.v2/env/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì bitsandbytes: 0.48.2\n",
      "‚úì numpy: 2.2.6\n",
      "‚úì pandas: 2.3.3\n",
      "\n",
      "üéÆ GPU Check:\n",
      "‚ö†Ô∏è  CUDA not available - training will be slow on CPU\n",
      "\n",
      "üéâ All packages installed correctly! You're ready to go!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ullahi/Downloads/Training.v2/env/lib/python3.10/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# üîç VERIFICATION CELL - Run this after changing the kernel\n",
    "# This will verify that all packages are installed correctly\n",
    "\n",
    "import sys\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üìç Python location: {sys.executable}\")\n",
    "print(\"\\nüì¶ Checking installed packages...\\n\")\n",
    "\n",
    "packages_to_check = [\n",
    "    ('accelerate', '1.1.0'),\n",
    "    ('peft', '0.13.2'),\n",
    "    ('torch', '2.5.0'),\n",
    "    ('transformers', None),\n",
    "    ('progressbar', None),\n",
    "    ('bitsandbytes', None),\n",
    "    ('numpy', None),\n",
    "    ('pandas', None),\n",
    "]\n",
    "\n",
    "all_good = True\n",
    "for package, expected_version in packages_to_check:\n",
    "    try:\n",
    "        if package == 'progressbar':\n",
    "            import progressbar\n",
    "            version = progressbar.__version__ if hasattr(progressbar, '__version__') else 'installed'\n",
    "        else:\n",
    "            mod = __import__(package)\n",
    "            version = mod.__version__\n",
    "        \n",
    "        if expected_version and version != expected_version:\n",
    "            print(f\"‚ö†Ô∏è  {package}: {version} (expected {expected_version})\")\n",
    "        else:\n",
    "            print(f\"‚úì {package}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"‚ùå {package}: NOT INSTALLED\")\n",
    "        all_good = False\n",
    "\n",
    "# Check CUDA availability\n",
    "import torch\n",
    "print(\"\\nüéÆ GPU Check:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì CUDA is available\")\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  CUDA not available - training will be slow on CPU\")\n",
    "\n",
    "if all_good:\n",
    "    print(\"\\nüéâ All packages installed correctly! You're ready to go!\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some packages are missing. Please re-run the setup cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3d",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h2 id=\"define-model\">Define the Model</h2>\n",
    "<p>Now that the environment is set up, let's define which pre-trained model we will use for fine-tuning. We will use <a href=\"https://huggingface.co/RaphaelMourad/Mistral-DNA-v1-17M-hg38\"><code class=\"language-plaintext highlighter-rouge\">Mistral-DNA-v1-17M-hg38</code></a>, a model that was pre-trained on the entire Human Genome. It contains approximately 17 million parameters and was trained using the Human Genome assembly GRCh38 on sequences of 10,000 bases (10K):</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_name=\"RaphaelMourad/Mistral-DNA-v1-17M-hg38\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<ol>\n",
    "<li>What is <code style=\"color: inherit\">accelerate</code>?</li>\n",
    "<li>What is <code style=\"color: inherit\">peft</code>?</li>\n",
    "<li>What is <code style=\"color: inherit\">torch</code>?</li>\n",
    "<li>What is <code style=\"color: inherit\">transformers</code>?</li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">accelerate</code> is a library by <a href=\"https://huggingface.co/\">Hugging Face</a> ‚Äì a platform that provides tools and resources for building, training, and deploying machine learning models ‚Äì designed to simplify the process of training and deploying machine learning models across different hardware environments. It provides tools to optimize performance on GPUs, TPUs, and other accelerators, making it easier to scale models efficiently.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p>The PEFT (Parameter-Efficient Fine-Tuning) Python library, developed by Hugging Face, is a tool designed to efficiently adapt large pretrained models to various downstream tasks without the need to fine-tune all of the model‚Äôs parameters. By focusing on a small subset of parameters, PEFT significantly reduces computational and storage costs, making it feasible to fine-tune large language models (LLMs) on consumer-grade hardware. The library integrates seamlessly with the Hugging Face ecosystem, including Transformers, Diffusers, and Accelerate, enabling streamlined model training and inference. PEFT supports techniques like LoRA (Low-Rank Adaptation) and prompt tuning, and it can be combined with quantization to further optimize resource usage. Its open-source nature fosters collaboration and accessibility, allowing developers to customize models for specific applications quickly and efficiently.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">torch</code>, also known as PyTorch, it is an open-source machine learning library developed by Facebook‚Äôs AI Research lab. It provides a flexible platform for building and training neural networks, with a focus on tensor computations and automatic differentiation.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">transformers</code> is a library by Hugging Face that provides implementations of state-of-the-art transformer models for natural language processing (NLP). It includes pre-trained models and tools for fine-tuning, making it easier to apply transformers to various NLP tasks.</p>\n",
    "</li>\n",
    "</ol>\n",
    "</details>\n",
    "</blockquote>\n",
    "<h2 id=\"import-python-libraries\">Import Python libraries</h2>\n",
    "<p>Let‚Äôs now import them.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from pathlib import Path\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from progressbar import ProgressBar\n",
    "from random import randrange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"comment\" style=\"border: 2px solid #ffecc1; margin: 1em 0.2em\">\n",
    "<div class=\"box-title comment-title\" id=\"comment-versions\"><i class=\"far fa-comment-dots\" aria-hidden=\"true\" ></i> Comment: Versions</div>\n",
    "<p>This tutorial has been tested with following versions:</p>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">numpy</code> = 1.19 (and not 1.2)</li>\n",
    "<li><code style=\"color: inherit\">transformers</code> &gt; 4.47.1</li>\n",
    "</ul>\n",
    "<p>You can check the versions with:</p>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div><pre style=\"color: inherit; background: transparent\"><code style=\"color: inherit\">np.__version__\n",
    "transformers.__version__\n",
    "</code></pre></div>  </div>\n",
    "</blockquote>\n",
    "<h1 id=\"configure-fine-tuning\">Configure fine-tuning</h1>\n",
    "<h2 id=\"check-and-configure-available-resources\">Check and configure available resources</h2>\n",
    "<p>We select the appropriate device (CUDA-enabled GPU if available) for running PyTorch operations</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>Let‚Äôs check the GPU usage and RAM:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>We now set an environment variable that configures how PyTorch manages CUDA memory allocations</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-11",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h2 id=\"specify-settings-for-quantization\">Specify settings for quantization</h2>\n",
    "<p>Quantization is a technique used in machine learning and signal processing to reduce the precision of numerical values, typically to decrease memory usage and computational requirements. This process is particularly useful when working with large models as it allows them to be deployed on hardware with limited resources without significantly sacrificing performance.</p>\n",
    "<p>Here, we use <code style=\"color: inherit\">BitsAndBytesConfig</code> to configure a 4-bit quantization. Using 4-bit precision reduces the memory footprint of the model, which is particularly useful for very large models that might not fit into GPU memory otherwise:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-13",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-1\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the parameters?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">load_in_4bit=True</code></li>\n",
    "<li><code style=\"color: inherit\">bnb_4bit_use_double_quant=True</code></li>\n",
    "<li><code style=\"color: inherit\">bnb_4bit_compute_dtype=torch.bfloat16</code></li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-1\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-1\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">load_in_4bit=True</code>: Specifies that the model should be loaded with 4-bit quantization. Using 4-bit precision reduces the memory footprint of the model, which is particularly useful for very large models that might not fit into GPU memory otherwise.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">bnb_4bit_use_double_quant=True</code>: enables double quantization, which means that the quantization constants from the first quantization are quantized again. This further reduces the memory footprint, although it may introduce additional computational overhead.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">bnb_4bit_compute_dtype=torch.bfloat16</code>: sets the compute data type to bfloat16 (Brain Floating Point 16-bit format). Using bfloat16 can provide a good balance between computational efficiency and numerical stability, especially on hardware that supports this format, such as certain GPUs and TPUs.</p>\n",
    "</li>\n",
    "</ol>\n",
    "</details>\n",
    "</blockquote>\n",
    "<h2 id=\"configure-accelerate\">Configure Accelerate</h2>\n",
    "<p>Now, we will configure the <a href=\"https://huggingface.co/docs/accelerate/en/index\">Hugging Face Accelerate library</a> to optimize the training process for large models using Fully Sharded Data Parallel (FSDP). This setup is crucial for efficiently utilizing GPU resources and enabling distributed training across multiple devices.</p>\n",
    "<p>First, we need to configure the FSDP plugin, which will manage how model parameters and optimizer states are sharded across GPUs. This configuration helps in reducing memory usage and allows for the training of larger models.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-15",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-2\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the parameters?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False)</code>?</li>\n",
    "<li><code style=\"color: inherit\">optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False)</code>?</li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-2\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-2\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False)</code>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">FullStateDictConfig</code>: Configures how the model‚Äôs state dictionary (parameters) is managed.</li>\n",
    "<li><code style=\"color: inherit\">offload_to_cpu=True</code>: Specifies that the model‚Äôs parameters should be offloaded to CPU memory when not in use. This helps free up GPU memory, especially useful when working with large models.</li>\n",
    "<li><code style=\"color: inherit\">rank0_only=False</code>: Indicates that the state dictionary operations (like saving and loading) are not restricted to the rank 0 process. This allows all processes to participate in these operations, which can be beneficial for distributed training setups.</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li><code style=\"color: inherit\">optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False)</code>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">FullOptimStateDictConfig</code>: Configures how the optimizer‚Äôs state dictionary is managed.</li>\n",
    "<li><code style=\"color: inherit\">offload_to_cpu=True</code>: Similar to the model‚Äôs state dictionary, this setting offloads the optimizer states to CPU memory when not in use, further reducing GPU memory usage.</li>\n",
    "<li><code style=\"color: inherit\">rank0_only=False</code>: Allows all processes to handle the optimizer state dictionary operations, ensuring that the optimizer states are managed efficiently across the distributed setup.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "</details>\n",
    "</blockquote>\n",
    "<p>Next, we initialize the Accelerator from the Hugging Face Accelerate library, integrating the FSDP plugin for seamless distributed training:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-17",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>By passing the FSDP plugin to the <code style=\"color: inherit\">Accelerator</code>, we enable sharded data parallelism, which efficiently manages model and optimizer states across multiple GPUs.</p>\n",
    "<p>With this configuration, the <code style=\"color: inherit\">Accelerator</code> will handle the complexities of distributed training, allowing us to focus on developing and experimenting with our models. This setup is particularly beneficial when working with large-scale models and limited GPU resources, as it optimizes memory usage and enables faster training times.</p>\n",
    "<h2 id=\"configure-lora-for-parameter-efficient-fine-tuning\">Configure LoRA for Parameter-Efficient Fine-Tuning</h2>\n",
    "<p>We will configure the LoRA (Low-Rank Adaptation) settings for parameter-efficient fine-tuning of a large language model. LoRA is a technique that allows us to fine-tune only a small number of additional parameters while keeping the original model weights frozen, making it highly efficient for adapting large models to specific tasks.</p>\n",
    "<p>We use the <code style=\"color: inherit\">LoraConfig</code> class to define the settings for LoRA. This configuration specifies how the low-rank adaptations are applied to the model.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-19",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-3\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the parameters?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">r=16</code>?</li>\n",
    "<li><code style=\"color: inherit\">lora_alpha=16</code>?</li>\n",
    "<li><code style=\"color: inherit\">lora_dropout=0.05</code>?</li>\n",
    "<li><code style=\"color: inherit\">bias=\"none\"</code>?</li>\n",
    "<li><code style=\"color: inherit\">task_type=\"SEQ_CLS\"</code>?</li>\n",
    "<li><code style=\"color: inherit\">target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]</code>?</li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-3\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-3\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">r=16</code>: This parameter specifies the rank of the low-rank matrices used in the adaptation. A higher rank allows the model to capture more complex patterns but also increases the number of trainable parameters.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">lora_alpha=16</code>: This scaling factor controls the magnitude of the updates applied by the low-rank matrices. It helps balance the influence of the adaptations relative to the original model weights.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">lora_dropout=0.05</code>: Dropout is applied to the low-rank matrices during training to prevent overfitting. A dropout rate of 0.05 means that 5% of the elements are randomly set to zero during each training step.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">bias=\"none\"</code>: This setting specifies that no bias parameters are added to the low-rank adaptations. Other options include ‚Äúall‚Äù to add biases to all layers or ‚Äúlora_only‚Äù to add biases only to the LoRA layers.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">task_type=\"SEQ_CLS\"</code>: This indicates that the model is being fine-tuned for a sequence classification task. Other task types might include ‚ÄúCAUSAL_LM‚Äù for causal language modeling or ‚ÄúSEQ_2_SEQ_LM‚Äù for sequence-to-sequence tasks.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\"]</code>: This list specifies the modules within the model architecture to which the LoRA adaptations will be applied. These modules are typically the attention layers in transformer models:</p>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">\"q_proj\"</code>: query projections</li>\n",
    "<li><code style=\"color: inherit\">\"k_proj\"</code>: key projections</li>\n",
    "<li><code style=\"color: inherit\">\"v_proj\"</code>: value projections</li>\n",
    "<li><code style=\"color: inherit\">\"o_proj\"</code>: output projections</li>\n",
    "<li><code style=\"color: inherit\">\"gate_proj\"</code>: gating projections in some architectures.</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ol>\n",
    "</details>\n",
    "</blockquote>\n",
    "<p>By configuring LoRA in this way, we can efficiently adapt a large pretrained model to a specific task with minimal computational overhead, making it feasible to fine-tune on consumer-grade hardware. This approach is particularly useful for tasks like text classification, sentiment analysis, or any other application where we need to specialize a general-purpose language model.</p>\n",
    "<h2 id=\"configure-training-arguments\">Configure Training Arguments</h2>\n",
    "<p>Let‚Äôs now set up the training arguments using the <code style=\"color: inherit\">TrainingArguments</code> class from the Hugging Face Transformers library. These arguments define the training configuration, including hyperparameters and settings for saving and evaluating the model.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-21",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end = False,\n",
    "    metric_for_best_model=\"accuracy\", \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-4\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the parameters?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">output_dir=\"./results\"</code></li>\n",
    "<li><code style=\"color: inherit\">evaluation_strategy=\"epoch\"</code></li>\n",
    "<li><code style=\"color: inherit\">save_strategy=\"epoch\"</code></li>\n",
    "<li><code style=\"color: inherit\">learning_rate=1e-5</code></li>\n",
    "<li><code style=\"color: inherit\">per_device_train_batch_size=16</code></li>\n",
    "<li><code style=\"color: inherit\">per_device_eval_batch_size=16</code></li>\n",
    "<li><code style=\"color: inherit\">num_train_epochs=5</code></li>\n",
    "<li><code style=\"color: inherit\">weight_decay=0.01</code></li>\n",
    "<li><code style=\"color: inherit\">bf16=True</code></li>\n",
    "<li><code style=\"color: inherit\">report_to=\"none\"</code></li>\n",
    "<li><code style=\"color: inherit\">load_best_model_at_end=True</code></li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-4\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-4\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">output_dir=\"./results\"</code>: Specifies the directory where the model predictions and checkpoints will be saved.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">evaluation_strategy=\"epoch\"</code>: The model will be evaluated at the end of each epoch. This allows for monitoring the model‚Äôs progress and adjusting the training process as needed.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">save_strategy=\"epoch\"</code>: The model checkpoints will be saved at the end of each epoch.  This ensures that checkpoints are available for each complete pass through the dataset.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">learning_rate=1e-5</code>: Sets the initial learning rate for the optimizer. This rate determines how much the model‚Äôs weights are updated during training.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">per_device_train_batch_size=16</code>: The number of samples per device (e.g., GPU) to load for training.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">per_device_eval_batch_size=16</code>: The number of samples per device to load for evaluation.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">num_train_epochs=5</code>: The total number of training epochs. An epoch is one complete pass through the training dataset.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">weight_decay=0.01</code>: Applies L2 regularization to the model weights to prevent overfitting.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">bf16=True</code>: Enables mixed precision training using bfloat16, which can speed up training and reduce memory usage on compatible hardware.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">report_to=\"none\"</code>: Disables reporting to external services like WandB or TensorBoard. If you want to track metrics, you can set this to ‚Äúwandb‚Äù, ‚Äútensorboard‚Äù, etc.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">load_best_model_at_end=True</code>: Ensures that the best model based on evaluation metrics is loaded at the end of training.</p>\n",
    "</li>\n",
    "</ol>\n",
    "</details>\n",
    "</blockquote>\n",
    "<p>These settings provide a balanced configuration for training a model efficiently while ensuring that the best version of the model is saved and can be used for further evaluation or deployment. Adjust these parameters based on your specific use case and available computational resources.</p>\n",
    "<h1 id=\"prepare-the-tokenizer\">Prepare the tokenizer</h1>\n",
    "<p>We will now set up the tokenizer to convert DNA sequences into numerical tokens that the model can process. The tokenizer is a crucial component in preparing the data for model training and inference: it transforms raw text into a format that can be processed by machine learning models.</p>\n",
    "<p>We use the <code style=\"color: inherit\">AutoTokenizer</code> class from the Hugging Face Transformers library to load a pre-trained tokenizer. We specify the pre-trained model from which to load the tokenizer. This should match the model you plan to use for training or inference. This tokenizer will be configured to handle DNA sequences efficiently.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-23",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    model_max_length=200,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-5\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the parameters?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">model_max_length=200</code></li>\n",
    "<li><code style=\"color: inherit\">padding_side=\"right\"</code></li>\n",
    "<li><code style=\"color: inherit\">use_fast=True</code></li>\n",
    "<li><code style=\"color: inherit\">trust_remote_code=True</code></li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-5\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-5\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">model_max_length=200</code>: Sets the maximum length of the tokenized sequences. Sequences longer than this will be truncated, and shorter ones will be padded.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">padding_side=\"right\"</code>: Specifies that padding should be added to the right side of the sequences. This ensures that all sequences in a batch have the same length.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">use_fast=True</code>: Enables the use of the fast tokenizer implementation, which is optimized for speed and is suitable for most use cases.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">trust_remote_code=True</code>: Allows the tokenizer to execute custom code from the model repository, which may be necessary for some models that require specific preprocessing steps.</p>\n",
    "</li>\n",
    "</ol>\n",
    "</details>\n",
    "</blockquote>\n",
    "<p>By configuring the tokenizer in this way, we ensure that our DNA sequences are properly tokenized and formatted for input into the model. This step is essential for preparing our data for efficient and effective model training and evaluation.</p>\n",
    "<p>Let‚Äôs now tailor the tokenizer to better suit our specific use case, ensuring that the model processes sequences accurately and efficiently. Special tokens play a crucial role in defining how sequences are processed and interpreted by the model. Here, we sets:</p>\n",
    "<ul>\n",
    "<li>the end-of-sequence (EOS) token, which indicates the end of a sequence. It is essential for tasks where the model needs to generate sequences or understand where a sequence ends.</li>\n",
    "<li>the padding (PAD) token, which is used to pad sequences to a uniform length within a batch. Padding ensures that all sequences in a batch have the same length, which is necessary for efficient processing during training and inference</li>\n",
    "</ul>\n",
    "<div class=\"language-plaintext highlighter-rouge\"><div><pre style=\"color: inherit; background: transparent\"><code style=\"color: inherit\">tokenizer.eos_token = \"[EOS]\"\n",
    "tokenizer.pad_token = \"[PAD]\"\n",
    "</code></pre></div></div>\n",
    "<h1 id=\"prepare-data\">Prepare data</h1>\n",
    "<p>To finetune the model, we must provide a dataset to train the model. We will the data with the 1st transcription factor (<code class=\"language-plaintext highlighter-rouge\">tf0</code>) in mouse from {% cite zhou2024dnabert2efficientfoundationmodel %}. The data is stored on <a href=\"https://github.com/raphaelmourad/Mistral-DNA\">GitHub</a>.</p>\n",
    "<h2 id=\"get-data\">Get data</h2>\n",
    "<p>Let‚Äôs get the data for from GitHub:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-25",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Mistral-DNA' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/raphaelmourad/Mistral-DNA.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>We now need to uncompress the labeled data:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-27",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "!tar -xf Mistral-DNA/data/GUE.tar.xz -C Mistral-DNA/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>We change the current working directory to the <code style=\"color: inherit\">Mistral-DNA</code> folder.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-29",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/biocorelab/BIX/projects/genomic-fm-course/workshop/jupyter-notebooks/fine-tuning-exercise/Training/Mistral-DNA\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"Mistral-DNA/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>Let‚Äôs define experience and path to data variables</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-31",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "expe = \"tf/0\"\n",
    "data_path = f\"data/GUE/{ expe }\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h2 id=\"prepare-datasets-for-training-and-validation\">Prepare Datasets for Training and Validation</h2>\n",
    "<p>We now need to set up the datasets required for training and validating. Properly preparing these datasets is crucial for ensuring that the model finetunes effectively and generalizes well to new data.</p>\n",
    "<p>We will use the files <code style=\"color: inherit\">data_path</code> folder we just defined:</p>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">train.csv</code> for training</li>\n",
    "<li><code style=\"color: inherit\">dev.csv</code> for validation</li>\n",
    "</ul>\n",
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-6\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>How is the content of each file?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">train.csv</code></li>\n",
    "<li><code style=\"color: inherit\">dev.csv</code></li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-6\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-6\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<p>The 2 files are CSV files with 2 columns (<code style=\"color: inherit\">sequence</code> and <code style=\"color: inherit\">label</code>) and different number of rows:</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">train.csv</code>: 32,379 rows.</li>\n",
    "<li><code style=\"color: inherit\">dev.csv</code>: 1,000 rows</li>\n",
    "</ol>\n",
    "<p>Values in <code style=\"color: inherit\">label</code> are:</p>\n",
    "<ul>\n",
    "<li><code style=\"color: inherit\">0</code>: The DNA sequence in <code style=\"color: inherit\">sequence</code> column does not bind to the 1st transcription factor.</li>\n",
    "<li><code style=\"color: inherit\">1</code>: The DNA sequence in <code style=\"color: inherit\">sequence</code> column binds to the transcription factor.</li>\n",
    "</ul>\n",
    "</details>\n",
    "</blockquote>\n",
    "<p>Before we proceed we import some classes and functions from <code style=\"color: inherit\">scriptPython/function.py</code>:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-33",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "### LOAD FUNCTIONS MODULE\n",
    "import sys\n",
    "sys.path.append(\"scriptPython/\")\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>We use the <code style=\"color: inherit\">SupervisedDataset</code> class to load and prepare the datasets. This class handles the tokenization and formatting of the data, making it ready for model training and evaluation.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-35",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Perform single sequence classification...\n",
      "WARNING:root:Perform single sequence classification...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SupervisedDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_path=Path(data_path) / \"train.csv\",\n",
    "    kmer=-1,\n",
    ")\n",
    "val_dataset = SupervisedDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_path=Path(data_path) / \"dev.csv\",\n",
    "    kmer=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-7\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What does <code style=\"color: inherit\">kmer=-1</code>?</p>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-7\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-7\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<p>This parameter is used to specify the length of k-mers (substrings of length k) to be considered in the dataset. A value of -1 typically means that no k-mer splitting is applied, and the sequences are processed as they are.</p>\n",
    "</details>\n",
    "</blockquote>\n",
    "<h2 id=\"configure-data-collation\">Configure Data Collation</h2>\n",
    "<p>A data collator ensures that sequences are properly padded and formatted, which is crucial for optimizing the training process.</p>\n",
    "<p>We‚Äôll use the <code style=\"color: inherit\">DataCollatorForSupervisedDataset</code> class to handle the collation of tokenized data. This collator will manage padding and ensure that all sequences in a batch are of uniform length.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-37",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h1 id=\"load-and-configure-the-model-for-sequence-classification\">Load and Configure the Model for Sequence Classification</h1>\n",
    "<p>Let‚Äôs now load the pre-trained model, a model originally trained for large language modeling tasks, not specifically for classification. To adapt it for our binary classification task, we will add a new classification head on top of the existing architecture. This head will consist of a single neuron that connects to the output of the language model, enabling it to classify whether a DNA sequence binds to a transcription factor (label <code style=\"color: inherit\">1</code>) or not (label <code style=\"color: inherit\">0</code>).</p>\n",
    "<p>This additional layer, or <strong>classification head</strong>, is a simple neural network layer that takes the high-level features extracted by the language model and maps them to our binary classification output. It learns to weigh these features appropriately to make accurate predictions for our specific task.</p>\n",
    "<p>We use the <code style=\"color: inherit\">AutoModelForSequenceClassification</code> class from the Hugging Face Transformers library to load the pre-trained model and set it up for our specific classification task:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-39",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model=\u001b[43mtransformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/biocorelab/BIX/projects/genomic-fm-course/workshop/jupyter-notebooks/fine-tuning-exercise/Training/.venv-genomic-llm-course/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/biocorelab/BIX/projects/genomic-fm-course/workshop/jupyter-notebooks/fine-tuning-exercise/Training/.venv-genomic-llm-course/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/biocorelab/BIX/projects/genomic-fm-course/workshop/jupyter-notebooks/fine-tuning-exercise/Training/.venv-genomic-llm-course/lib/python3.11/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/biocorelab/BIX/projects/genomic-fm-course/workshop/jupyter-notebooks/fine-tuning-exercise/Training/.venv-genomic-llm-course/lib/python3.11/site-packages/transformers/modeling_utils.py:5468\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5465\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5468\u001b[39m         _error_msgs, disk_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5469\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/biocorelab/BIX/projects/genomic-fm-course/workshop/jupyter-notebooks/fine-tuning-exercise/Training/.venv-genomic-llm-course/lib/python3.11/site-packages/transformers/modeling_utils.py:843\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     disk_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/biocorelab/BIX/projects/genomic-fm-course/workshop/jupyter-notebooks/fine-tuning-exercise/Training/.venv-genomic-llm-course/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/biocorelab/BIX/projects/genomic-fm-course/workshop/jupyter-notebooks/fine-tuning-exercise/Training/.venv-genomic-llm-course/lib/python3.11/site-packages/transformers/modeling_utils.py:750\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[39m\n\u001b[32m    748\u001b[39m param = param[...]\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     param = \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[32m    752\u001b[39m     param = param.contiguous()\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model=transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    output_hidden_states=False,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-8\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the parameters?</p>\n",
    "<ol>\n",
    "<li><code style=\"color: inherit\">num_labels=2</code></li>\n",
    "<li><code style=\"color: inherit\">output_hidden_states=False</code></li>\n",
    "<li><code style=\"color: inherit\">quantization_config=bnb_config</code></li>\n",
    "<li><code style=\"color: inherit\">device_map=\"auto\"</code></li>\n",
    "<li><code style=\"color: inherit\">trust_remote_code=True</code></li>\n",
    "</ol>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-8\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-8\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<ol>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">num_labels=2</code>: Sets the number of output labels to 2, corresponding to the binary classification task (binding or not binding to transcription factors).</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">output_hidden_states=False</code>: Indicates that the model should not output hidden states. This is typically set to False unless you need access to the intermediate representations for further analysis.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">quantization_config=bnb_config</code>: Applies predefined quantization configuration to the model, which helps reduce memory usage and enables efficient training on consumer-grade hardware.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">device_map=\"auto\"</code>: Automatically determines the best device placement for the model‚Äôs layers, optimizing for available hardware (e.g., GPUs). If it finds a GPU, it will use a GPU. If there‚Äôs no GPU, it will not use the GPU</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><code style=\"color: inherit\">trust_remote_code=True</code>: Allows the model to execute custom code from the model repository, which may be necessary for certain architectures or preprocessing steps.</p>\n",
    "</li>\n",
    "</ol>\n",
    "</details>\n",
    "</blockquote>\n",
    "<p>To ensure that the model correctly handles padding tokens, we need to align the padding token configuration between the model and the tokenizer. This step is crucial for maintaining consistency during training and inference, especially when dealing with sequences of varying lengths:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<h1 id=\"initialize-the-trainer\">Initialize the Trainer</h1>\n",
    "<p>We can now set up the <code style=\"color: inherit\">Trainer</code> to manage the training and evaluation process of our model. The <code style=\"color: inherit\">Trainer</code> class simplifies the training loop, handling many of the complexities involved in training deep learning models.</p>\n",
    "<p>We first need to attach the LoRA adapter to the model:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-43",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config, adapter_name=\"lora_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>Let‚Äôs now set up the <code style=\"color: inherit\">Trainer</code>:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-45",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-9\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What do the <code style=\"color: inherit\">callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]</code> parameter?</p>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-9\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-9\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<p>It adds an early stopping mechanism to the training process. This mechanism is designed to halt training when the model‚Äôs performance on the validation set stops improving, helping to prevent overfitting and conserve computational resources.</p>\n",
    "<p>How Early Stopping Works?</p>\n",
    "<p><strong>Purpose</strong>: The primary goal of early stopping is to capture the model parameters when the loss reaches its minimum value during training. This is crucial because, after a certain point, continued training may lead to overfitting, where the model starts to perform worse on unseen data.</p>\n",
    "<p><strong>Patience Parameter</strong>: The <code style=\"color: inherit\">early_stopping_patience=3</code> setting specifies that training should continue for three additional epochs after the model‚Äôs performance on the validation set stops improving. This ‚Äúpatience‚Äù period helps mitigate the effects of noise in the training process. Noise can cause temporary fluctuations in the loss, making it seem like the model has reached a local minimum when further training might yield better results.</p>\n",
    "<p><strong>Process</strong>: During training, the loss is monitored at each epoch. If the loss does not decrease for three consecutive epochs, training is stopped. However, if a better model with a lower loss is found within those three epochs, training continues. This approach ensures that the model has truly reached a robust local minimum, rather than being prematurely halted due to noise.</p>\n",
    "<p>By incorporating early stopping with a patience of three epochs, you balance the need to find an optimal model with the risk of overfitting, ultimately leading to more efficient and effective training outcomes.</p>\n",
    "</details>\n",
    "</blockquote>\n",
    "<p>Ffor distributed training, where multiple GPUs or nodes are used to accelerate the training process, it is essential to do:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-47",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "trainer.local_rank=training_args.local_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-48",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>The <code style=\"color: inherit\">local_rank</code> parameter identifies the rank of the current process within its local node, enabling coordinated communication and synchronization between processes. This setup is crucial for managing tasks such as gradient synchronization and data partitioning, ensuring that each process operates on the correct portion of the model or dataset. By assigning the local rank from <code style=\"color: inherit\">training_args</code> to the <code style=\"color: inherit\">Trainer</code>, we facilitate efficient and scalable training, leveraging the full computational power of multi-GPU environments.</p>\n",
    "<h1 id=\"start-the-training\">Start the training</h1>\n",
    "<p>Let‚Äôs start the training process for our model using the trainer.train() method:</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-49",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10120' max='10120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10120/10120 20:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Matthews Correlation</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.673200</td>\n",
       "      <td>0.667697</td>\n",
       "      <td>0.633000</td>\n",
       "      <td>0.632770</td>\n",
       "      <td>0.265644</td>\n",
       "      <td>0.632762</td>\n",
       "      <td>0.632882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.604500</td>\n",
       "      <td>0.602135</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.692261</td>\n",
       "      <td>0.384835</td>\n",
       "      <td>0.692701</td>\n",
       "      <td>0.692134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>0.571767</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.703331</td>\n",
       "      <td>0.409052</td>\n",
       "      <td>0.705797</td>\n",
       "      <td>0.703263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.553700</td>\n",
       "      <td>0.559777</td>\n",
       "      <td>0.721000</td>\n",
       "      <td>0.719742</td>\n",
       "      <td>0.441065</td>\n",
       "      <td>0.721515</td>\n",
       "      <td>0.719554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.552600</td>\n",
       "      <td>0.556095</td>\n",
       "      <td>0.723000</td>\n",
       "      <td>0.721675</td>\n",
       "      <td>0.445134</td>\n",
       "      <td>0.723651</td>\n",
       "      <td>0.721488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10120, training_loss=0.6012172329567167, metrics={'train_runtime': 1238.1296, 'train_samples_per_second': 130.754, 'train_steps_per_second': 8.174, 'total_flos': 436038100531200.0, 'train_loss': 0.6012172329567167, 'epoch': 5.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>After launching <code style=\"color: inherit\">trainer.train()</code>, we can notice that the training process is significantly faster compared to training a model from scratch seen in <a href=\"{% link topics/statistics/tutorials/genomic-llm-pretraining/tutorial.md %}\">‚Äù‚Äù tutorial</a>. This efficiency is due to the use of a pre-trained model, which has already undergone extensive training on large datasets using powerful computational resources. For example, pre-training a model on even a small portion of the human genome can take dozens of hours, but fine-tuning this model on a specific task, such as classifying DNA sequences, is much quicker. Fine-tuning leverages the pre-trained model‚Äôs foundational knowledge, allowing you to adapt it to new tasks with a smaller, labeled dataset. This approach not only saves time but also reduces the need for extensive computational power. By downloading a pre-trained model from platforms like Hugging Face and fine-tuning it on a local machine with a modest GPU, we can achieve high performance with minimal overhead, making advanced modeling techniques accessible for a wide range of applications.</p>\n",
    "<h1 id=\"evaluate-model-performance\">Evaluate Model Performance</h1>\n",
    "<p>After successfully training the model, the next essential step is to evaluate its performance on a test dataset. This evaluation process is crucial for understanding how well the model generalizes to new, unseen data and for assessing its readiness for real-world applications.</p>\n",
    "<blockquote class=\"comment\" style=\"border: 2px solid #ffecc1; margin: 1em 0.2em\">\n",
    "<div class=\"box-title comment-title\" id=\"comment\"><i class=\"far fa-comment-dots\" aria-hidden=\"true\" ></i> Comment</div>\n",
    "<p>If finetuning is too long, you can stop the training.</p>\n",
    "</blockquote>\n",
    "<p>The test data is stored in <code style=\"color: inherit\">data_path/test.csv</code>, we prepare it as for training and validation data.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-51",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Perform single sequence classification...\n"
     ]
    }
   ],
   "source": [
    "test_dataset = SupervisedDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    data_path=Path(data_path) / \"test.csv\",\n",
    "    kmer=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>We then use the <code style=\"color: inherit\">trainer.evaluate()</code> method. This methods is designed to assess the model‚Äôs performance on a specified dataset, typically the test dataset, which contains data that the model has not encountered during training.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-53",
   "metadata": {
    "attributes": {
     "classes": [
      "> <comment-title>Transcription factors</comment-title>"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.evaluate(eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "<p>The method computes various evaluation metrics, such as accuracy, precision, recall, and F1 score, depending on the task and the configuration specified in <code style=\"color: inherit\">compute_metrics</code>. These metrics provide a comprehensive view of the model‚Äôs performance, highlighting its strengths and weaknesses.</p>\n",
    "<p>The Trainer uses the <code style=\"color: inherit\">data_collator</code> to ensure that the test data is properly formatted and padded, maintaining consistency with the training process. This consistency is crucial for accurate evaluation.</p>\n",
    "<p>The evaluation results are stored in the <code style=\"color: inherit\">results</code> variable, which contains the computed metrics. We can analyze these <code style=\"color: inherit\">results</code> to gain insights into the model‚Äôs performance and make informed decisions about further improvements or deployment.</p>\n",
    "<blockquote class=\"question\" style=\"border: 2px solid #8A9AD0; margin: 1em 0.2em\">\n",
    "<div class=\"box-title question-title\" id=\"question-10\"><i class=\"far fa-question-circle\" aria-hidden=\"true\" ></i> Question</div>\n",
    "<p>What is stored in <code style=\"color: inherit\">results</code>? How do you interpret this information?</p>\n",
    "<br/><details style=\"border: 2px solid #B8C3EA; margin: 1em 0.2em;padding: 0.5em; cursor: pointer;\"><summary>üëÅ View solution</summary>\n",
    "<div class=\"box-title solution-title\" id=\"solution-10\"><button class=\"gtn-boxify-button solution\" type=\"button\" aria-controls=\"solution-10\" aria-expanded=\"true\"><i class=\"far fa-eye\" aria-hidden=\"true\" ></i> <span>Solution</span><span class=\"fold-unfold fa fa-minus-square\"></span></button></div>\n",
    "<p><code style=\"color: inherit\">results</code> provides a comprehensive overview of the model‚Äôs performance on the evaluation dataset with:</p>\n",
    "<ol>\n",
    "<li>\n",
    "<p><strong>eval_loss (0.424961)</strong>: This metric represents the loss value calculated on the evaluation dataset. Lower values indicate better model performance.</p>\n",
    "<p>A loss of 0.425 suggests that the model is reasonably well-fitted to the data, though the specific interpretation depends on the context and the loss function used (e.g., cross-entropy for classification tasks).</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>eval_accuracy (0.804000)</strong>: Accuracy measures the proportion of correctly predicted instances out of the total instances.</p>\n",
    "<p>An accuracy of 80.4% indicates that the model correctly predicted the class for 80.4% of the samples in the evaluation dataset.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>eval_f1 (0.800838)</strong>: The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both concerns.</p>\n",
    "<p>An F1 score of 0.801 suggests a good balance between precision and recall, indicating that the model performs well in both identifying positive cases and minimizing false positives and negatives.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>eval_matthews_correlation (0.628276)</strong>: The Matthews Correlation Coefficient (MCC) is a measure of the quality of binary classifications, taking into account true and false positives and negatives.</p>\n",
    "<p>An MCC of 0.628 indicates a moderate to strong correlation between the predicted and actual classes, suggesting the model is performing better than random guessing.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>eval_precision (0.824614)</strong>: Precision is the ratio of correctly predicted positive observations to the total predicted positives.</p>\n",
    "<p>A precision of 82.5% means that out of all the instances predicted as positive, 82.5% were actually positive.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>eval_recall (0.804000)</strong>: Recall (or sensitivity) is the ratio of correctly predicted positive observations to all observations in the actual class.</p>\n",
    "<p>A recall of 80.4% indicates that the model correctly identified 80.4% of all actual positive cases.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>eval_runtime (6.548800)</strong>: The total time taken to evaluate the model on the dataset.</p>\n",
    "<p>A runtime of 6.55 seconds provides insight into the computational efficiency of the evaluation process.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>eval_samples_per_second (152.699000)</strong>: The number of samples processed per second during evaluation.</p>\n",
    "<p>Processing 152.7 samples per second indicates the efficiency of the evaluation pipeline.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>eval_steps_per_second (9.620000)</strong>: The number of evaluation steps completed per second.</p>\n",
    "<p>Completing 9.62 steps per second reflects the speed of the evaluation process.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p><strong>epoch (3.000000)</strong>: The number of training epochs completed before this evaluation.</p>\n",
    "</li>\n",
    "</ol>\n",
    "<p>The evaluation was conducted after 3 epochs of training, providing context for the model‚Äôs learning progress.</p>\n",
    "</details>\n",
    "</blockquote>\n",
    "<h1 id=\"conclusion\">Conclusion</h1>\n",
    "<p>In this tutorial, we explored the process of fine-tuning a large language model (LLM) for DNA sequence classification. By following the steps outlined, you have learned how to leverage pre-trained models to achieve efficient and effective classification of DNA sequences, specifically focusing on their binding affinity to transcription factors.</p>\n",
    "<p>We began by configuring the fine-tuning process, ensuring that available computational resources were optimally utilized. This included specifying settings for quantization, configuring Accelerate for distributed training, and implementing LoRA for parameter-efficient fine-tuning. These steps were crucial for maximizing performance and minimizing computational overhead.</p>\n",
    "<p>Next, we prepared the tokenizer and data, ensuring that DNA sequences were properly tokenized and formatted for model input. We created datasets for training and validation, and configured data collation to handle batch processing efficiently.</p>\n",
    "<p>We then loaded and configured the model for sequence classification, adding a classification head to adapt the pre-trained model to our specific task. With the model and data prepared, we initialized the <code style=\"color: inherit\">Trainer</code>, which streamlined the training process by managing the training loop, evaluation, and checkpointing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01b21ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5215451717376709, 'eval_accuracy': 0.761, 'eval_f1': 0.7586343755144168, 'eval_matthews_correlation': 0.5325432812342925, 'eval_precision': 0.7716497570764241, 'eval_recall': 0.761, 'eval_runtime': 3.2758, 'eval_samples_per_second': 305.272, 'eval_steps_per_second': 19.232, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-ending-cell",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Key Points\n",
    "\n",
    "- Fine-tuning pre-trained LLMs reduces training time and computational needs, making advanced research accessible.\n",
    "- Techniques like LoRA enable fine-tuning on modest hardware, broadening access to powerful models.\n",
    "- Rigorous testing on unseen data confirms a model's practical applicability and reliability.\n",
    "\n",
    "# Congratulations on successfully completing this tutorial!\n",
    "\n",
    "Please [fill out the feedback on the GTN website](https://training.galaxyproject.org/training-material/topics/statistics/tutorials/genomic-llm-finetuning/tutorial.html#feedback) and check there for further resources!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-inference-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
